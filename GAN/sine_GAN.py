from keras.models import Sequential
from keras.layers import Dense, Dropout, LeakyReLU

import numpy as np
from numpy.random import randint, uniform
import matplotlib.pyplot as plt

SAMPLE_LEN = 64  # number N of points where a curve is sampled
SAMPLE_SIZE = 32768  # number of curves in the training set
X_MIN = -5.0  # least ordinate where to sample
X_MAX = 5.0  # last ordinate where to sample
Y_MIN, Y_MAX = -1.0, 1.0

X_COORDS = np.linspace(X_MIN, X_MAX, SAMPLE_LEN)

# The training set
SAMPLE = np.zeros((SAMPLE_SIZE, SAMPLE_LEN))
for i in range(0, SAMPLE_SIZE):
    b = uniform(0.5, 2.0) #scale
    c = uniform(np.math.pi) #phase
    SAMPLE[i] = np.array([np.sin(b*x + c) for x in X_COORDS])

# Create a figure with three subplots.
fig, axes = plt.subplots(2)
# Increase the space between subplots.
plt.subplots_adjust(hspace=0.5, wspace=0.5)

# Set the axis labels and title.
#axes[0].set_xlabel("x")
#axes[0].set_ylabel("y")
axes[0].set_title("Original")
axes[0].plot(X_COORDS, SAMPLE[0])
axes[0].plot(X_COORDS, SAMPLE[1])
axes[0].plot(X_COORDS, SAMPLE[2])
#plt.show()

DROPOUT = Dropout(0.4)  # Empirical hyperparameter
discriminator = Sequential()
discriminator.add(Dense(SAMPLE_LEN, activation="relu"))
discriminator.add(DROPOUT)
discriminator.add(Dense(SAMPLE_LEN * 2, activation="relu"))
discriminator.add(DROPOUT)
discriminator.add(Dense(1, activation="sigmoid"))
discriminator.compile(optimizer="adam",
                      loss="binary_crossentropy",
                      metrics=["accuracy"])
BATCH_SIZE = 128
EPOCHS = 32

ONES = np.ones((BATCH_SIZE//2))
ZEROS = np.zeros((BATCH_SIZE//2))
ONEZEROS = (ONES, ZEROS)
NOISE = uniform(Y_MIN, Y_MAX, size=(SAMPLE_SIZE, SAMPLE_LEN))

for i in range(EPOCHS):
    n = randint(0, SAMPLE_SIZE, size=BATCH_SIZE // 2)
    x = np.concatenate((SAMPLE[n], NOISE[n]))
    y = np.concatenate(ONEZEROS)
    dummy, acc = discriminator.train_on_batch(x, y)
    print(f"  {i:3}  |   {acc}")

generator = Sequential()
generator.add(Dense(SAMPLE_LEN, activation="relu"))
generator.add(Dense(256, activation="relu"))
generator.add(Dense(SAMPLE_LEN, activation = "tanh"))
generator.compile(optimizer="adam", loss="mse", metrics=["accuracy"])

gan = Sequential()
gan.add(generator)
discriminator.trainable = False
gan.add(discriminator)
gan.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

EPOCHS = 64

print("epoch | dis. loss | dis. acc | gen. loss | gen. acc")
print("------+-----------+----------+-----------+----------")

for e in range(EPOCHS):
    for k in range(SAMPLE_SIZE // BATCH_SIZE):
        n = randint(0, SAMPLE_SIZE, size=BATCH_SIZE // 2)
        x = np.concatenate((SAMPLE[n], generator.predict(NOISE[n], verbose=False)))
        y = np.concatenate(ONEZEROS)
        discriminator.trainable = True
        d_loss, d_acc = discriminator.train_on_batch(x, y)
        discriminator.trainable = False
        g_loss, g_acc = gan.train_on_batch(NOISE[n], ONES)

    print(f" {e:03n} |  {d_loss:.5f}  |  {d_acc:.5f} |  {g_loss:.5f}  |  {g_acc:.5f}")

# Plots a curve generated by the GAN
x = uniform(Y_MIN, Y_MAX, size=(1, SAMPLE_LEN))
y = generator.predict(x)[0]

axes[1].set_title("Generator Prediction")
axes[1].plot(X_COORDS, y)
#plt.show()

x = uniform(Y_MIN, Y_MAX, size = (1, SAMPLE_LEN))
y = generator.predict(x)[0]

axes[1].plot(X_COORDS, x[0])
axes[1].plot(X_COORDS, y)
plt.show()

discriminator.predict(y.reshape(1, SAMPLE_LEN))
